{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive mounted successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data directory from GitHub repository\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create target directory\n",
        "os.makedirs('/content/drive/MyDrive/ILR-IR', exist_ok=True)\n",
        "\n",
        "# Clone repository and copy data\n",
        "try:\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/pooyaht/ILR-IR.git', '/tmp/ILR-IR'], check=True)\n",
        "    print('Repository cloned successfully')\n",
        "    \n",
        "    # Copy data using shell command\n",
        "    !cp -vr /tmp/ILR-IR/data /content/drive/MyDrive/ILR-IR/\n",
        "    \n",
        "except:\n",
        "    print('Git clone failed. Please manually download the data from: https://github.com/pooyaht/ILR-IR/tree/main/data')\n",
        "finally:\n",
        "    # Change to the project directory\n",
        "    os.chdir('/content/drive/MyDrive/ILR-IR/')\n",
        "    print(f'Changed working directory to: {os.getcwd()}')\n",
        "    print('Data downloaded and setup completed successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: pyHGT/data.py\n",
        "# Source: ./pyHGT/data.py\n",
        "\n",
        "import json, os\n",
        "import math, copy, time\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import dill\n",
        "from functools import partial\n",
        "import multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Graph():\n",
        "    def __init__(self):\n",
        "        super(Graph, self).__init__()\n",
        "        '''\n",
        "        self.t_r_id_p_dict_train-->time:relaton:quadid:path list(2\u7ef4\uff0c\u7b2c\u4e00\u884c\u4e3a\u6b63\u786e\u7684triple\u4e4b\u95f4\u7684\u6240\u6709path)\n",
        "        self.t_max_num-->\u6bcf\u4e2a\u65f6\u9699\u5185\u6240\u6709triple\u4e4b\u95f4\u7684\u6700\u5927\u8def\u5f84\u6570\n",
        "        '''\n",
        "        '''\n",
        "\n",
        "        self.t_r_id_p_dict_train = defaultdict(lambda: {})\n",
        "        self.t_r_id_p_dict_valid = defaultdict(lambda: {})\n",
        "        self.t_r_id_p_dict_test = defaultdict(lambda: {})\n",
        "\n",
        "        self.t_paths_train = defaultdict(lambda: [])\n",
        "        self.t_paths_valid = defaultdict(lambda: [])\n",
        "        self.t_paths_test = defaultdict(lambda: [])\n",
        "\n",
        "        self.t_paths_len_train = defaultdict(lambda: [])\n",
        "        self.t_paths_len_valid = defaultdict(lambda: [])\n",
        "        self.t_paths_len_test = defaultdict(lambda: [])\n",
        "\n",
        "        self.t_max_num_train = {}\n",
        "        self.t_max_num_valid = {}\n",
        "        self.t_max_num_test = {}\n",
        "        '''\n",
        "\n",
        "        self.t_r_id_p_dict = defaultdict(lambda: {})\n",
        "        self.t_r_id_target_dict = defaultdict(lambda: {})\n",
        "\n",
        "        self.r_copy = defaultdict(lambda: {})\n",
        "        #self.r_copy_t = defaultdict(lambda: {})\n",
        "\n",
        "\n",
        "        self.t_paths = defaultdict(lambda: [])\n",
        "\n",
        "        self.t_paths_len = defaultdict(lambda: [])\n",
        "\n",
        "        self.t_paths_time = defaultdict(lambda: [])\n",
        "        self.t_paths_m_time = defaultdict(lambda: [])\n",
        "    \n",
        "class RenameUnpickler(dill.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        renamed_module = module\n",
        "        if module == \"GPT_GNN.data\" or module == 'data':\n",
        "            renamed_module = \"pyHGT.data\"\n",
        "        return super(RenameUnpickler, self).find_class(renamed_module, name)\n",
        "\n",
        "\n",
        "def renamed_load(file_obj):\n",
        "    return RenameUnpickler(file_obj).load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: pyHGT/model.py\n",
        "# Source: ./pyHGT/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        return device, True\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        return device, True\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        return device, False\n",
        "\n",
        "\n",
        "DEVICE, HAS_ACCELERATION = get_device()\n",
        "CUDA = HAS_ACCELERATION\n",
        "\n",
        "\n",
        "class RelTemporalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, n_hid, max_len=4020, dropout=0.2):\n",
        "        super(RelTemporalEncoding, self).__init__()\n",
        "        position = torch.arange(0., max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, n_hid, 2) *\n",
        "                             -(math.log(10000.0) / n_hid))\n",
        "        emb = nn.Embedding(max_len, n_hid)\n",
        "        emb.weight.data[:, 0::2] = torch.sin(\n",
        "            position * div_term) / math.sqrt(n_hid)\n",
        "        emb.weight.data[:, 1::2] = torch.cos(\n",
        "            position * div_term) / math.sqrt(n_hid)\n",
        "        emb.requires_grad_(False)\n",
        "        self.emb = emb\n",
        "        self.lin = nn.Linear(n_hid, n_hid)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return x + self.lin(self.emb(t))\n",
        "\n",
        "\n",
        "class TypeGAT(nn.Module):\n",
        "    def __init__(self, num_e, num_r, relation_embeddings, out_dim):\n",
        "        super(TypeGAT, self).__init__()\n",
        "\n",
        "        self.num_e = num_e\n",
        "        self.num_r = num_r\n",
        "        self.in_dim = relation_embeddings.shape[1]\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.pad = torch.zeros(1, self.out_dim)\n",
        "        if HAS_ACCELERATION:\n",
        "            self.pad = self.pad.to(DEVICE)\n",
        "\n",
        "        self.relation_embeddings = nn.Parameter(relation_embeddings)\n",
        "        self.emb = RelTemporalEncoding(self.out_dim)\n",
        "\n",
        "        self.gru = nn.GRU(input_size=self.in_dim,\n",
        "                          hidden_size=self.out_dim, batch_first=True)\n",
        "\n",
        "        self.gru.reset_parameters()\n",
        "\n",
        "    def forward2(self, path_index, batch_relation, paths, paths_time, lengths, path_r, path_neg_index, batch_his_r):\n",
        "        r_inp = self.relation_embeddings\n",
        "\n",
        "        # update relations r<-path\n",
        "        pad_r = torch.cat((r_inp, self.pad), dim=0)\n",
        "        emb = pad_r[paths]\n",
        "        emb = self.emb(emb, paths_time)  # temporal information\n",
        "\n",
        "        lengths_cpu = lengths.cpu()\n",
        "        packed = pack_padded_sequence(\n",
        "            emb, lengths_cpu, batch_first=True, enforce_sorted=False).to(paths.device)\n",
        "        _, hidden = self.gru(packed)\n",
        "\n",
        "        path_emb = torch.cat((self.pad, hidden.squeeze(0)), dim=0)\n",
        "        del emb, packed, paths\n",
        "\n",
        "        pad_r = torch.cat((F.normalize(r_inp, dim=1),\n",
        "                          self.pad.to(r_inp.device)), dim=0)\n",
        "        # pad_r = F.normalize(pad_r, dim=1)\n",
        "        path_emb = F.normalize(path_emb, dim=1)\n",
        "\n",
        "        # batch*num_paths\n",
        "        scores = torch.mm(path_emb, pad_r[batch_relation].t()).t()\n",
        "        mask = torch.zeros((scores.size(0), scores.size(1))).to(scores.device)\n",
        "        m_index = min(path_index.size(1), mask.size(1))\n",
        "        mask = mask.scatter(1, path_index[:, 0:m_index], 1)\n",
        "        max_score, max_id = torch.max(scores * mask, 1)\n",
        "\n",
        "        scores_r = torch.mm(pad_r, pad_r.t())[batch_relation]\n",
        "        his_score = torch.mean(torch.diagonal(\n",
        "            scores_r[:, batch_his_r], dim1=0, dim2=1).t(), 1)\n",
        "\n",
        "        # scores = torch.mm(path_emb, pad_r[batch_relation[0]].unsqueeze(1)).squeeze(1)\n",
        "        # max_score, max_id = torch.max(scores[path_index], 1)\n",
        "\n",
        "        # scores_r = torch.mm(pad_r, pad_r[batch_relation[0]].unsqueeze(1)).squeeze(1)\n",
        "        # his_score = torch.mean(scores_r[batch_his_r], 1)\n",
        "\n",
        "        # score = max_score+his_score\n",
        "        score = max_score\n",
        "\n",
        "        return score, path_emb[path_neg_index], pad_r[path_r]\n",
        "\n",
        "    def test(self, path_index, batch_relation, paths, lengths, paths_time, batch_his_r):\n",
        "        r_inp = self.relation_embeddings\n",
        "\n",
        "        pad = torch.zeros(1, self.out_dim)\n",
        "\n",
        "        # update relations r<-path\n",
        "        pad_r = torch.cat((r_inp, pad.to(r_inp.device)), dim=0)\n",
        "        emb = pad_r[paths]\n",
        "        emb = self.emb(emb, paths_time)  # temporal information\n",
        "        lengths_cpu = lengths.cpu()\n",
        "        packed = pack_padded_sequence(\n",
        "            emb, lengths_cpu, batch_first=True, enforce_sorted=False)\n",
        "        _, hidden = self.gru(packed)\n",
        "        path_emb = torch.cat(\n",
        "            (self.pad.to(r_inp.device), hidden.squeeze(0)), dim=0)\n",
        "\n",
        "        del emb, packed, paths\n",
        "        pad_r = torch.cat((F.normalize(r_inp, dim=1),\n",
        "                          pad.to(r_inp.device)), dim=0)\n",
        "        path_emb = F.normalize(path_emb, dim=1)\n",
        "\n",
        "        scores = torch.mm(\n",
        "            path_emb, pad_r[batch_relation[0]].unsqueeze(1)).squeeze(1)\n",
        "        max_score, max_id = torch.max(scores[path_index], 1)\n",
        "\n",
        "        scores_r = torch.mm(\n",
        "            pad_r, pad_r[batch_relation[0]].unsqueeze(1)).squeeze(1)\n",
        "        his_score = torch.mean(scores_r[batch_his_r], 1)\n",
        "\n",
        "        score = max_score + his_score\n",
        "        # score = his_score\n",
        "\n",
        "        return score\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: dataprocess.py\n",
        "# Source: ./dataprocess.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "from more_itertools import flatten\n",
        "from sklearn.utils import shuffle\n",
        "from operator import itemgetter\n",
        "import dill\n",
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    args = argparse.ArgumentParser()\n",
        "    args.add_argument(\"-data\", \"--data\",\n",
        "                      default=\"./data/ICEWS14_forecasting\", help=\"data directory\")\n",
        "    args.add_argument(\"-state\", \"--state\",\n",
        "                      default=\"train\", help=\"train or test\")\n",
        "    args.add_argument(\"-neg_ratio\", \"--ratio\",\n",
        "                      default=1, type=int, help=\"training neg ratio\")\n",
        "    args.add_argument(\"-his_len\", \"--his_len\",\n",
        "                      default=50, type=int, help=\"1 hop historial relations\",)\n",
        "\n",
        "    # Notebook-compatible argument parsing\n",
        "    import sys\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        # Running in notebook - use defaults\n",
        "        args = argparse.Namespace(\n",
        "            data='./data/ICEWS14_forecasting',\n",
        "            state='train',\n",
        "            ratio=1,\n",
        "            his_len=13\n",
        "        )\n",
        "    else:\n",
        "        # Running as script - use command line args\n",
        "        args = args.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "# Notebook-compatible argument setup\n",
        "import sys\n",
        "if 'ipykernel' in sys.modules:\n",
        "    # Running in notebook - create args with defaults\n",
        "    import argparse\n",
        "    args = argparse.Namespace(\n",
        "        data='./data/ICEWS14_forecasting',\n",
        "        state='train',\n",
        "        ratio=1,\n",
        "        his_len=13\n",
        "    )\n",
        "else:\n",
        "    # Running as script - use command line args\n",
        "    args = parse_args()\n",
        "\n",
        "\n",
        "def all_simple_edge_paths(G, source, target, cutoff=None):\n",
        "    if source not in G:\n",
        "        raise nx.NodeNotFound(\"source node %s not in graph\" % source)\n",
        "    if target in G:\n",
        "        targets = {target}\n",
        "    else:\n",
        "        try:\n",
        "            targets = set(target)\n",
        "        except TypeError:\n",
        "            raise nx.NodeNotFound(\"target node %s not in graph\" % target)\n",
        "    if source in targets:\n",
        "        return []\n",
        "    if cutoff is None:\n",
        "        cutoff = len(G) - 1\n",
        "    if cutoff < 1:\n",
        "        return []\n",
        "    if G.is_multigraph():\n",
        "        for simp_path in _all_simple_edge_paths_multigraph(G, source, targets, cutoff):\n",
        "            yield simp_path\n",
        "    else:\n",
        "        for simp_path in _all_simple_paths_graph(G, source, targets, cutoff):\n",
        "            yield list(zip(simp_path[:-1], simp_path[1:]))\n",
        "\n",
        "\n",
        "def _all_simple_edge_paths_multigraph(G, source, targets, cutoff):\n",
        "    if not cutoff or cutoff < 1:\n",
        "        return []\n",
        "    visited = [source]\n",
        "    stack = [iter(G.edges(source, keys=True))]\n",
        "\n",
        "    while stack:\n",
        "        children = stack[-1]\n",
        "        child = next(children, None)\n",
        "        if child is None:\n",
        "            stack.pop()\n",
        "            visited.pop()\n",
        "        elif len(visited) < cutoff:\n",
        "            if child[1] in targets:\n",
        "                yield visited[1:] + [child]\n",
        "            if child[1] not in [v[0] for v in visited[1:]]:\n",
        "                visited.append(child)\n",
        "                stack.append(iter(G.edges(child[1], keys=True)))\n",
        "        else:  # len(visited) == cutoff:\n",
        "            for (u, v, k) in [child] + list(children):\n",
        "                if v in targets:\n",
        "                    yield visited[1:] + [(u, v, k)]\n",
        "            stack.pop()\n",
        "            visited.pop()\n",
        "\n",
        "\n",
        "def _all_simple_paths_graph(G, source, targets, cutoff):\n",
        "    if cutoff < 1:\n",
        "        return\n",
        "\n",
        "    visited = [source]\n",
        "    stack = [iter(G[source])]\n",
        "\n",
        "    while stack:\n",
        "        children = stack[-1]\n",
        "        child = next(children, None)\n",
        "\n",
        "        if child is None:\n",
        "            stack.pop()\n",
        "            visited.pop()\n",
        "        elif len(visited) < cutoff:\n",
        "            if child in targets:\n",
        "                yield visited + [child]\n",
        "\n",
        "            if child not in visited:\n",
        "                visited.append(child)\n",
        "                stack.append(iter(G[child]))\n",
        "        else:\n",
        "            for neighbor in [child] + list(children):\n",
        "                if neighbor in targets and neighbor not in visited:\n",
        "                    yield visited + [neighbor]\n",
        "            stack.pop()\n",
        "            visited.pop()\n",
        "\n",
        "\n",
        "def parse_line(line):\n",
        "    line = line.strip().split()\n",
        "    e1, relation, e2 = line[0].strip(), line[1].strip(), line[2].strip()\n",
        "    return e1, relation, e2\n",
        "\n",
        "\n",
        "def build_data(path, num_r):\n",
        "\n",
        "    t_quads = {}\n",
        "    t_quads_re = {}\n",
        "\n",
        "    all_triples = set()\n",
        "    quads_id = {}\n",
        "    with open(os.path.join(path, 'data.txt'), 'r') as fr:\n",
        "        times = set()\n",
        "        for i, line in enumerate(fr):\n",
        "            line_split = line.split()\n",
        "            time = int(line_split[3])\n",
        "            times.add(time)\n",
        "\n",
        "            e1, relation, e2 = int(line_split[0]), int(\n",
        "                line_split[1]), int(line_split[2])\n",
        "\n",
        "            all_triples.add((e1, relation, e2))\n",
        "            all_triples.add((e2, relation+num_r, e1))\n",
        "\n",
        "            t_quads.setdefault(time, []).append((e1, relation, e2))\n",
        "            t_quads_re.setdefault(time, []).append((e2, relation+num_r, e1))\n",
        "\n",
        "        all_triples = list(all_triples)\n",
        "        for i, triple in enumerate(all_triples):\n",
        "            quads_id[triple] = i\n",
        "\n",
        "    all_times = list(times)\n",
        "    all_times.sort()\n",
        "\n",
        "    t_quadid = {}\n",
        "    t_quadid_re = {}\n",
        "    for t in all_times:\n",
        "        for i in t_quads[t]:\n",
        "            t_quadid.setdefault(t, []).append(quads_id[i])\n",
        "        for j in t_quads_re[t]:\n",
        "            t_quadid_re.setdefault(t, []).append(quads_id[j])\n",
        "    print(\"number of triples ->\", len(all_triples))\n",
        "\n",
        "    return t_quadid, t_quadid_re, all_triples, all_times\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self, args, all_triples, num_e, num_r):\n",
        "\n",
        "        self.all_triples = all_triples\n",
        "        self.num_e = num_e\n",
        "        self.num_r = num_r\n",
        "\n",
        "    def get_neg_triples(self, args, all_times, t_quads, test_idx):\n",
        "        quads_select = {}  # quad_id, cur_id, shortest length\n",
        "        quads_neg = {}\n",
        "        G = nx.Graph()\n",
        "        if args.state == 'train':\n",
        "            # if flag == 0:\n",
        "            times = range(test_idx)\n",
        "\n",
        "        else:\n",
        "            times = range(test_idx, len(all_times))\n",
        "\n",
        "            keys_his = all_times[:test_idx]\n",
        "\n",
        "            quads_his = list(itemgetter(*keys_his)(t_quads))\n",
        "            triples_his = np.array(self.all_triples)[\n",
        "                list(set(flatten(quads_his)))]\n",
        "            G.add_edges_from(triples_his[:, [0, 2]])\n",
        "\n",
        "        for idx in tqdm(times):\n",
        "\n",
        "            quads = t_quads[all_times[idx]]\n",
        "            quad_id = []\n",
        "            neg_len = defaultdict(lambda: [])\n",
        "            pre = 0\n",
        "            valid_triples = np.array(self.all_triples)[quads].tolist()\n",
        "\n",
        "            for i, quad in enumerate(quads):\n",
        "                triple = self.all_triples[quad]\n",
        "\n",
        "                try:\n",
        "                    pred = nx.predecessor(G, triple[0], triple[2], 3)\n",
        "                    if len(pred) > 0:\n",
        "                        length = nx.shortest_path_length(\n",
        "                            G, triple[0], triple[2])\n",
        "                    else:\n",
        "                        # current facts\n",
        "                        # G.add_edge(triple[0], triple[2])\n",
        "                        continue\n",
        "                except:\n",
        "                    pass\n",
        "                else:\n",
        "                    if length < 4:\n",
        "                        quad_id.append([quad, pre, i, length])\n",
        "                        pre = i\n",
        "\n",
        "                        paths_len = nx.single_source_shortest_path_length(\n",
        "                            G, triple[0], 3)\n",
        "                        del paths_len[triple[0]]\n",
        "                        if args.state == 'train':\n",
        "                            ids = shuffle(list(paths_len.keys()))[\n",
        "                                :min(len(paths_len), 3)]\n",
        "                            for target in ids:\n",
        "                                l = paths_len[target]\n",
        "                                if [triple[0], triple[1], target] not in valid_triples:\n",
        "                                    neg_len[quad].append([target, l])\n",
        "                        else:\n",
        "                            for target, l in paths_len.items():\n",
        "                                if [triple[0], triple[1], target] not in valid_triples:\n",
        "                                    neg_len[quad].append([target, l])\n",
        "\n",
        "                # current facts\n",
        "                # G.add_edge(triple[0], triple[2])\n",
        "            if len(quad_id) != 0 and len(neg_len) != 0:\n",
        "                quads_select[idx] = quad_id\n",
        "                quads_neg[idx] = neg_len\n",
        "\n",
        "            triples_his = np.array(self.all_triples)[t_quads[all_times[idx]]]\n",
        "            G.add_edges_from(triples_his[:, [0, 2]])\n",
        "\n",
        "        return quads_select, quads_neg\n",
        "\n",
        "    def get_path_test(self, args, G, s, targets, lens, cur_time, num_r):\n",
        "        target_pid = defaultdict(list)\n",
        "        target_his_pid = defaultdict(list)  # s,o\u4e4b\u95f4\u7684\u5386\u53f2\u4ea4\u4e92\u5173\u7cfb\n",
        "\n",
        "        try:\n",
        "            paths = []\n",
        "            for i in range(len(targets)):\n",
        "                c_graph = G.subgraph([s, targets[i]])\n",
        "                sG = G.subgraph(c_graph)\n",
        "                paths.extend(list(all_simple_edge_paths(sG, s, targets[i], 3)))\n",
        "                # paths.extend(list(all_simple_edge_paths(sG, s, targets[i], max(lens[i],2))))\n",
        "                # paths.extend(list(all_simple_edge_paths(sG, s, targets[i], lens[i])))\n",
        "\n",
        "            path_len = [len(path) for path in paths]\n",
        "            p_id = np.argsort(path_len)\n",
        "\n",
        "            tar_dict = {}\n",
        "        except:  # \u53ef\u7701\u7565\u9519\u8bef\u7c7b\u578b\n",
        "            print('sample error')\n",
        "        else:  # \u6ca1\u6709\u9519\u8bef\u7684\u8bdd\u7ee7\u7eed\u6267\u884c\u4e0b\u9762\u7684\u7a0b\u5e8f\n",
        "            if len(paths) != 0:\n",
        "                # print(paths)\n",
        "                for id in p_id:\n",
        "                    path = paths[id]\n",
        "\n",
        "                    t = np.array(path)[-1][1]\n",
        "                    pa = np.array(path)[:, 2]\n",
        "                    pa_t = [cur_time - G.edges[p]['time']\n",
        "                            for p in path]  # \u76f8\u5bf9\u65f6\u95f4\n",
        "\n",
        "                    if t not in tar_dict.keys():\n",
        "                        tar_dict[t] = [len(pa), max(pa_t)]\n",
        "                    elif max(pa_t) < tar_dict[t][1]:\n",
        "                        tar_dict[t][1] = max(pa_t)\n",
        "                    elif len(pa) > tar_dict[t][0] and max(pa_t) > tar_dict[t][1]:\n",
        "                        continue\n",
        "\n",
        "                    if len(pa) == 3:\n",
        "                        pl = self.pathlen_3[(pa[0], pa_t[0])][(\n",
        "                            pa[1], pa_t[1])][(pa[2], pa_t[2])]\n",
        "                        if pl == 0:\n",
        "                            self.paths.append(pa)\n",
        "                            self.paths_time.append(pa_t)\n",
        "                            self.lengths.append(len(pa))\n",
        "                            self.paths_m_time.append(max(pa_t))\n",
        "                            target_pid[t].append(len(self.paths))\n",
        "                            self.pathlen_3[(pa[0], pa_t[0])][(pa[1], pa_t[1])][(\n",
        "                                pa[2], pa_t[2])] = len(self.paths)\n",
        "                        else:\n",
        "                            if pl not in target_pid[t]:\n",
        "                                target_pid[t].append(pl)\n",
        "                    elif len(pa) == 2:\n",
        "                        pl = self.pathlen_2[(pa[0], pa_t[0])][(pa[1], pa_t[1])]\n",
        "                        if pl == 0:\n",
        "                            self.paths.append(pa)\n",
        "                            self.paths_time.append(pa_t)\n",
        "                            self.lengths.append(len(pa))\n",
        "                            self.paths_m_time.append(max(pa_t))\n",
        "                            target_pid[t].append(len(self.paths))\n",
        "                            self.pathlen_2[(pa[0], pa_t[0])][(\n",
        "                                pa[1], pa_t[1])] = len(self.paths)\n",
        "                        else:\n",
        "                            if pl not in target_pid[t]:\n",
        "                                target_pid[t].append(pl)\n",
        "                    elif len(pa) == 1:\n",
        "                        pl = self.pathlen_1[(pa[0], pa_t[0])]\n",
        "                        if pa_t[0] <= args.his_len:\n",
        "                            target_his_pid[t].append(pa[0])\n",
        "                        if pl == 0:\n",
        "                            self.paths.append(pa)\n",
        "                            self.paths_time.append(pa_t)\n",
        "                            self.lengths.append(len(pa))\n",
        "                            self.paths_m_time.append(max(pa_t))\n",
        "                            target_pid[t].append(len(self.paths))\n",
        "                            self.pathlen_1[(pa[0], pa_t[0])] = len(self.paths)\n",
        "\n",
        "                        else:\n",
        "                            if pl not in target_pid[t]:\n",
        "                                target_pid[t].append(pl)\n",
        "        target_pid_sort = defaultdict(list)\n",
        "        for t in target_pid.keys():\n",
        "            if t not in target_his_pid.keys():\n",
        "                target_pid_sort[t] = [num_r * 2]\n",
        "            else:\n",
        "                target_pid_sort[t] = target_his_pid[t]\n",
        "\n",
        "        return target_pid, target_pid_sort\n",
        "\n",
        "    def get_iteration_batch(self, args, G, batch_quads, negs, quads_cur, cur_time, num_r):\n",
        "\n",
        "        self.paths = []\n",
        "        self.lengths = []\n",
        "        self.paths_time = []\n",
        "        self.paths_m_time = []\n",
        "\n",
        "        self.pathlen_1 = defaultdict(int)\n",
        "        self.pathlen_2 = defaultdict(lambda: defaultdict(int))\n",
        "        self.pathlen_3 = defaultdict(lambda:\n",
        "                                     defaultdict(lambda:\n",
        "                                                 defaultdict(int)))\n",
        "\n",
        "        paths_dict = defaultdict(lambda: defaultdict(list))\n",
        "        targets_dict = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        paths_dict_copy = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        for quad, pre, pid, length in tqdm(batch_quads):\n",
        "            target = []\n",
        "            lens = []\n",
        "            s, r, o = self.all_triples[quad]\n",
        "            # if pid >= 0:\n",
        "            #    edges = np.array(self.all_triples)[quads_cur[pre:pid]][:, [0, 2, 1]]\n",
        "            #    G.add_edges_from(edges, time=cur_time)\n",
        "\n",
        "            target.append(o)\n",
        "            lens.append(length)\n",
        "            neg = np.array(negs[quad])\n",
        "\n",
        "            if len(neg) > 0:\n",
        "                if args.state == 'train':\n",
        "                    neg = shuffle(negs[quad])\n",
        "                    neg_num = min(len(neg), args.ratio)\n",
        "                    t_l = np.array(neg[:neg_num])\n",
        "                    t_neg = t_l[:, [0]]\n",
        "                    target.extend(t_neg.reshape(-1).tolist())\n",
        "\n",
        "                    l_neg = t_l[:, [1]]\n",
        "                    lens.extend(l_neg.reshape(-1).tolist())\n",
        "                elif args.state == 'test':\n",
        "                    t_neg = neg[:, [0]]\n",
        "                    target.extend(t_neg.reshape(-1).tolist())\n",
        "\n",
        "                    l_neg = neg[:, [1]]\n",
        "                    lens.extend(l_neg.reshape(-1).tolist())\n",
        "\n",
        "            subnodes = []\n",
        "            subnodes.append(s)\n",
        "            subnodes.extend(target)\n",
        "            graph1 = G.subgraph(subnodes)\n",
        "            H = G.subgraph(list(graph1.nodes()))\n",
        "\n",
        "            target_pid, target_his_pid = self.get_path_test(\n",
        "                args, H, s, target, lens, cur_time, num_r)\n",
        "            if o not in target_pid.keys():\n",
        "                continue\n",
        "\n",
        "            # pos triple\n",
        "\n",
        "            paths_dict[r][quad].append(target_pid[o])\n",
        "            targets_dict[r][quad].append(o)\n",
        "\n",
        "            paths_dict_copy[r][quad].append(target_his_pid[o])\n",
        "            del target_pid[o], target_his_pid[o]\n",
        "\n",
        "            # neg_triples\n",
        "            if len(target_pid.keys()) == 0:\n",
        "                continue\n",
        "\n",
        "            paths_dict[r][quad].extend(list(target_pid.values()))\n",
        "            targets_dict[r][quad].extend(list(target_pid.keys()))\n",
        "\n",
        "            paths_dict_copy[r][quad].extend(list(target_his_pid.values()))\n",
        "\n",
        "        del self.pathlen_1, self.pathlen_2, self.pathlen_3\n",
        "\n",
        "        return paths_dict, targets_dict, self.paths, self.lengths, self.paths_time, paths_dict_copy, self.paths_m_time\n",
        "\n",
        "\n",
        "def main():\n",
        "    with open(os.path.join('{}'.format(args.data), 'stat.txt'), 'r') as fr:\n",
        "        for line in fr:\n",
        "            line_split = line.split()\n",
        "            num_e, num_r = int(line_split[0]), int(line_split[1])\n",
        "\n",
        "    t_quads, t_quads_re, all_triples, all_times = build_data(args.data, num_r)\n",
        "\n",
        "    with open(os.path.join('{}'.format(args.data), 'split.txt'), 'r') as fr:\n",
        "        for line in fr:\n",
        "            line_split = line.split()\n",
        "\n",
        "            valid_start = int(line_split[1].split(',')[0])\n",
        "\n",
        "            test_start = int(line_split[2].split(',')[0])\n",
        "\n",
        "    time_list = list(all_times)\n",
        "    valid_idx = time_list.index(valid_start)\n",
        "    test_idx = time_list.index(test_start)\n",
        "\n",
        "    Corpus_ = Corpus(args, all_triples, num_e, num_r)\n",
        "\n",
        "    graph_train = Graph()\n",
        "    print('sample')\n",
        "\n",
        "    if not os.path.exists(os.path.join(args.data + '/quads_select.pk')):\n",
        "        quads_select, quads_neg = Corpus_.get_neg_triples(\n",
        "            args, all_times, t_quads, test_idx)\n",
        "        dill.dump(quads_select, open(args.data + '/quads_select.pk', 'wb'))\n",
        "        dill.dump(quads_neg, open(args.data + '/quads_neg.pk', 'wb'))\n",
        "    else:\n",
        "        quads_select = renamed_load(\n",
        "            open(os.path.join(args.data + '/quads_select.pk'), 'rb'))\n",
        "        quads_neg = renamed_load(\n",
        "            open(os.path.join(args.data + '/quads_neg.pk'), 'rb'))\n",
        "    G = nx.MultiDiGraph()\n",
        "\n",
        "    print('train')\n",
        "\n",
        "    for idx in range(valid_idx):\n",
        "        if idx < 1:\n",
        "            continue\n",
        "        triples_his = np.array(all_triples)[t_quads[all_times[idx - 1]]]\n",
        "        G.add_edges_from(triples_his[:, [0, 2, 1]], time=idx-1)\n",
        "\n",
        "        triples_his_re = np.array(all_triples)[t_quads_re[all_times[idx - 1]]]\n",
        "        G.add_edges_from(triples_his_re[:, [0, 2, 1]], time=idx - 1)\n",
        "        quads_cur = t_quads[all_times[idx]]\n",
        "        if args.state == 'train':\n",
        "\n",
        "            try:\n",
        "                quads = quads_select[idx]\n",
        "                negs = quads_neg[idx]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            paths_dict, targets_dict, paths, lengths, paths_time, paths_dict_copy, paths_m_time = Corpus_.get_iteration_batch(\n",
        "                args, G, quads, negs, quads_cur, idx, num_r)\n",
        "            graph_train.t_r_id_p_dict[idx] = paths_dict\n",
        "            graph_train.t_r_id_target_dict[idx] = targets_dict\n",
        "            graph_train.t_paths[idx] = paths\n",
        "            graph_train.t_paths_len[idx] = lengths\n",
        "            graph_train.t_paths_time[idx] = paths_time\n",
        "            graph_train.t_paths_m_time[idx] = paths_m_time\n",
        "\n",
        "            graph_train.r_copy[idx] = paths_dict_copy\n",
        "            print(idx, len(lengths))\n",
        "    if args.state == 'train':\n",
        "        dill.dump(graph_train, open(\n",
        "            args.data + '/graph_preprocess_train.pk', 'wb'))\n",
        "        del graph_train\n",
        "\n",
        "    print('valid')\n",
        "    graph_valid = Graph()\n",
        "    for idx in range(valid_idx, test_idx):\n",
        "        triples_his = np.array(all_triples)[t_quads[all_times[idx - 1]]]\n",
        "        G.add_edges_from(triples_his[:, [0, 2, 1]], time=idx - 1)\n",
        "\n",
        "        triples_his_re = np.array(all_triples)[t_quads_re[all_times[idx - 1]]]\n",
        "        G.add_edges_from(triples_his_re[:, [0, 2, 1]], time=idx - 1)\n",
        "        if args.state == 'train':\n",
        "\n",
        "            quads_cur = t_quads[all_times[idx]]\n",
        "\n",
        "            try:\n",
        "                quads = quads_select[idx]\n",
        "                negs = quads_neg[idx]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            paths_dict, targets_dict, paths, lengths, paths_time, paths_dict_copy, paths_m_time = Corpus_.get_iteration_batch(\n",
        "                args, G, quads, negs, quads_cur, idx, num_r)\n",
        "            graph_valid.t_r_id_p_dict[idx] = paths_dict\n",
        "            graph_valid.t_r_id_target_dict[idx] = targets_dict\n",
        "\n",
        "            graph_valid.t_paths[idx] = paths\n",
        "            graph_valid.t_paths_len[idx] = lengths\n",
        "            graph_valid.t_paths_time[idx] = paths_time\n",
        "            graph_valid.t_paths_m_time[idx] = paths_m_time\n",
        "\n",
        "            graph_valid.r_copy[idx] = paths_dict_copy\n",
        "            print(idx, len(lengths))\n",
        "\n",
        "    if args.state == 'train':\n",
        "        dill.dump(graph_valid, open(\n",
        "            args.data + '/graph_preprocess_valid.pk', 'wb'))\n",
        "        del graph_valid\n",
        "\n",
        "    else:\n",
        "        graph_test = Graph()\n",
        "        if not os.path.exists(os.path.join(args.data + '/quads_select_test.pk')):\n",
        "            quads_select, quads_neg = Corpus_.get_neg_triples(\n",
        "                args, all_times, t_quads, test_idx)\n",
        "            dill.dump(quads_select, open(\n",
        "                args.data + '/quads_select_test.pk', 'wb'))\n",
        "            dill.dump(quads_neg, open(args.data + '/quads_neg_test.pk', 'wb'))\n",
        "        else:\n",
        "            quads_select = renamed_load(\n",
        "                open(os.path.join(args.data + '/quads_select_test.pk'), 'rb'))\n",
        "            quads_neg = renamed_load(\n",
        "                open(os.path.join(args.data + '/quads_neg_test.pk'), 'rb'))\n",
        "\n",
        "        quads_num = 0\n",
        "        quads_select_num = 0\n",
        "        quads_select_neg = 0\n",
        "        print('test')\n",
        "        for idx in range(test_idx, len(all_times)):\n",
        "            triples_his = np.array(all_triples)[t_quads[all_times[idx - 1]]]\n",
        "            G.add_edges_from(triples_his[:, [0, 2, 1]], time=idx - 1)\n",
        "            triples_his_re = np.array(all_triples)[\n",
        "                t_quads_re[all_times[idx - 1]]]\n",
        "            G.add_edges_from(triples_his_re[:, [0, 2, 1]], time=idx - 1)\n",
        "            quads_cur = t_quads[all_times[idx]]\n",
        "            quads_num = quads_num+len(quads_cur)\n",
        "\n",
        "            try:\n",
        "                quads = quads_select[idx]\n",
        "                negs = quads_neg[idx]\n",
        "                quads_select_num = quads_select_num + len(quads)\n",
        "                quads_select_neg = quads_select_neg + len(negs)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            paths_dict, targets_dict, paths, lengths, paths_time, paths_dict_copy, paths_m_time = Corpus_.get_iteration_batch(\n",
        "                args, G, quads, negs, quads_cur, idx, num_r)\n",
        "            graph_test.t_r_id_p_dict[idx] = paths_dict\n",
        "            graph_test.t_r_id_target_dict[idx] = targets_dict\n",
        "\n",
        "            graph_test.t_paths[idx] = paths\n",
        "            graph_test.t_paths_len[idx] = lengths\n",
        "            graph_test.t_paths_time[idx] = paths_time\n",
        "            graph_test.t_paths_m_time[idx] = paths_m_time\n",
        "\n",
        "            graph_test.r_copy[idx] = paths_dict_copy\n",
        "            print(idx, len(lengths))\n",
        "\n",
        "        del quads_select, quads_neg\n",
        "        print(\"select quads:\", quads_select_num)\n",
        "        print(\"select neg:\", quads_select_neg)\n",
        "        print(\"all quads:\", quads_num)\n",
        "\n",
        "        dill.dump(graph_test, open(\n",
        "            args.data + '/graph_preprocess_test.pk', 'wb'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: main_pre.py\n",
        "# Source: ./main_pre.py\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        print(\"Using Apple MPS\")\n",
        "        return device, True\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Using CUDA\")\n",
        "        return device, True\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"Using CPU\")\n",
        "        return device, False\n",
        "\n",
        "\n",
        "DEVICE, HAS_ACCELERATION = get_device()\n",
        "CUDA = HAS_ACCELERATION\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    args = argparse.ArgumentParser()\n",
        "\n",
        "    args.add_argument(\"-data\", \"--data\",\n",
        "                      default=\"./data/ICEWS14_forecasting\", help=\"data directory\")\n",
        "    args.add_argument('--dataset', type=str, default='ICEWS14_forecasting')\n",
        "    args.add_argument(\"-e_c\", \"--epochs_conv\", type=int,\n",
        "                      default=100, help=\"Number of epochs\")\n",
        "    args.add_argument(\"-w_conv\", \"--weight_decay_conv\", type=float,\n",
        "                      default=1e-6, help=\"L2 reglarization for conv\")\n",
        "    args.add_argument(\"-emb_size\", \"--embedding_size\", type=int,\n",
        "                      default=200, help=\"Size of embeddings (if pretrained not used)\")\n",
        "    args.add_argument(\"-l\", \"--lr\", type=float, default=1e-4)\n",
        "\n",
        "    # Notebook-compatible argument parsing\n",
        "    import sys\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        # Running in notebook - use defaults\n",
        "        args = argparse.Namespace(\n",
        "            data='./data/ICEWS14_forecasting',\n",
        "            state='train',\n",
        "            ratio=1,\n",
        "            his_len=13\n",
        "        )\n",
        "    else:\n",
        "        # Running as script - use command line args\n",
        "        args = args.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def save_model(model, name, folder_name, epoch):\n",
        "    print(\"Saving Model\")\n",
        "    torch.save(model.state_dict(),\n",
        "               (folder_name + \"trained\"+str(epoch)+\".pth\"))\n",
        "    print(\"Done saving Model\")\n",
        "\n",
        "\n",
        "def mkdirs(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "# Notebook-compatible argument setup\n",
        "import sys\n",
        "if 'ipykernel' in sys.modules:\n",
        "    # Running in notebook - create args with defaults\n",
        "    import argparse\n",
        "    args = argparse.Namespace(\n",
        "        data='./data/ICEWS14_forecasting',\n",
        "        state='train',\n",
        "        ratio=1,\n",
        "        his_len=13\n",
        "    )\n",
        "else:\n",
        "    # Running as script - use command line args\n",
        "    args = parse_args()\n",
        "\n",
        "mkdirs('./results/bestmodel/{}/conv'.format(args.dataset))\n",
        "mkdirs('./results/bestmodel/{}/gat'.format(args.dataset))\n",
        "model_state_file = './results/bestmodel/{}/'.format(args.dataset)\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    with open(os.path.join('{}'.format(args.data), 'stat.txt'), 'r') as fr:\n",
        "        for line in fr:\n",
        "            line_split = line.split()\n",
        "            num_e, num_r = int(line_split[0]), int(line_split[1])\n",
        "\n",
        "    relation_embeddings = np.random.randn(num_r * 2, args.embedding_size)\n",
        "    print(\"Initialised relations and entities randomly\")\n",
        "    return num_e, num_r, torch.FloatTensor(relation_embeddings)\n",
        "\n",
        "\n",
        "num_e, num_r, relation_embeddings = load_data(args)\n",
        "\n",
        "print(\"Initial relation dimensions {}\".format(relation_embeddings.size()))\n",
        "\n",
        "\n",
        "def list_to_array(x, pad):\n",
        "    dff = pd.concat([pd.DataFrame({'{}'.format(index): labels})\n",
        "                    for index, labels in enumerate(x)], axis=1)\n",
        "    return dff.fillna(pad).values.T.astype(int)\n",
        "\n",
        "\n",
        "def train_conv(args):\n",
        "    print(\"Defining model\")\n",
        "    model = TypeGAT(num_e, num_r*2, relation_embeddings, args.embedding_size)\n",
        "    if HAS_ACCELERATION:\n",
        "        model.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
        "\n",
        "    margin_loss = torch.nn.SoftMarginLoss()\n",
        "    cosine_loss = nn.CosineEmbeddingLoss(margin=0.0)\n",
        "\n",
        "    epoch_losses = []\n",
        "    print(\"Number of epochs {}\".format(args.epochs_conv))\n",
        "\n",
        "    graph_train = renamed_load(\n",
        "        open(os.path.join(args.data + '/graph_preprocess_train.pk'), 'rb'))\n",
        "\n",
        "    total_time_steps = len(graph_train.t_r_id_p_dict)\n",
        "    print(f\"Total time steps per epoch: {total_time_steps}\")\n",
        "\n",
        "    for epoch in range(args.epochs_conv):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EPOCH {epoch + 1}/{args.epochs_conv}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        epoch_loss = []\n",
        "\n",
        "        time_steps = list(graph_train.t_r_id_p_dict.items())\n",
        "        progress_bar = tqdm(\n",
        "            time_steps, desc=f\"Epoch {epoch+1}\", unit=\"timestep\")\n",
        "\n",
        "        batch_count = 0\n",
        "        loss_sum = 0.0\n",
        "\n",
        "        for step_idx, (t, r_dict) in enumerate(progress_bar):\n",
        "            step_start_time = time.time()\n",
        "\n",
        "            batch_values = []\n",
        "            batch_paths_id = []\n",
        "            batch_relation = []\n",
        "            batch_his_r = []\n",
        "\n",
        "            # for neg paths\n",
        "            path_r = []\n",
        "            path_neg_index = []\n",
        "\n",
        "            # Count total batches in this time step\n",
        "            total_relations = len(r_dict)\n",
        "            relations_processed = 0\n",
        "\n",
        "            for r, id_p in r_dict.items():\n",
        "                len_r = 0\n",
        "                p_neg_temp = []\n",
        "                for id, ps in id_p.items():\n",
        "                    len_r = len_r + len(ps)\n",
        "\n",
        "                    value = [-1] * len(ps)\n",
        "                    value[0] = 1\n",
        "                    batch_values.extend(value)\n",
        "                    batch_paths_id.extend(ps)\n",
        "\n",
        "                    batch_his_r.extend(graph_train.r_copy[t][r][id])\n",
        "                    if len(ps) > 1:\n",
        "                        p_neg_temp.extend(\n",
        "                            list(eval('['+str(ps[1:]).replace(\"[\", '').replace(\"]\", '')+']')))\n",
        "\n",
        "                batch_relation.extend([r]*len_r)\n",
        "                path_r.extend([r]*len(p_neg_temp))\n",
        "                path_neg_index.extend(p_neg_temp)\n",
        "\n",
        "                relations_processed += 1\n",
        "\n",
        "            if len(batch_values) > 0:\n",
        "                path_values = [-1]*len(path_r)\n",
        "                path_values = torch.FloatTensor(\n",
        "                    np.expand_dims(np.array(path_values), axis=1))\n",
        "                path_neg_index = torch.LongTensor(np.array(path_neg_index))\n",
        "                path_r = torch.LongTensor(np.array(path_r))\n",
        "\n",
        "                batch_paths_id = torch.LongTensor(\n",
        "                    list_to_array(batch_paths_id, 0))\n",
        "                batch_relation = torch.LongTensor(np.array(batch_relation))\n",
        "                batch_values = torch.FloatTensor(\n",
        "                    np.expand_dims(np.array(batch_values), axis=1))\n",
        "                batch_his_r = torch.LongTensor(\n",
        "                    list_to_array(batch_his_r, num_r*2))\n",
        "\n",
        "                paths = graph_train.t_paths[t]\n",
        "                paths_time = graph_train.t_paths_time[t]\n",
        "                lengths = graph_train.t_paths_len[t]\n",
        "\n",
        "                if len(paths) != 0:\n",
        "                    paths = pad_sequence([torch.LongTensor(np.array(p)) for p in paths], batch_first=True,\n",
        "                                         padding_value=num_r*2)\n",
        "                    paths_time = pad_sequence([torch.LongTensor(np.array(p)) for p in paths_time], batch_first=True,\n",
        "                                              padding_value=0)\n",
        "                else:\n",
        "                    paths = torch.LongTensor(np.array(paths))\n",
        "                    paths_time = torch.LongTensor(np.array(paths_time))\n",
        "                lengths = torch.LongTensor(np.array(lengths))\n",
        "\n",
        "                if HAS_ACCELERATION:\n",
        "                    batch_paths_id = Variable(batch_paths_id).to(DEVICE)\n",
        "                    batch_relation = Variable(batch_relation).to(DEVICE)\n",
        "                    batch_his_r = Variable(batch_his_r).to(DEVICE)\n",
        "                    paths = Variable(paths).to(DEVICE)\n",
        "                    paths_time = Variable(paths_time).to(DEVICE)\n",
        "                    lengths = Variable(lengths).to(DEVICE)\n",
        "                    path_r = Variable(path_r).to(DEVICE)\n",
        "                    path_neg_index = Variable(path_neg_index).to(DEVICE)\n",
        "                    batch_values = Variable(batch_values).to(DEVICE)\n",
        "                    path_values = Variable(path_values).to(DEVICE)\n",
        "                else:\n",
        "                    batch_paths_id = Variable(batch_paths_id)\n",
        "                    batch_relation = Variable(batch_relation)\n",
        "                    batch_his_r = Variable(batch_his_r)\n",
        "                    paths = Variable(paths)\n",
        "                    paths_time = Variable(paths_time)\n",
        "                    lengths = Variable(lengths)\n",
        "                    path_r = Variable(path_r)\n",
        "                    path_neg_index = Variable(path_neg_index)\n",
        "                    batch_values = Variable(batch_values)\n",
        "                    path_values = Variable(path_values)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                try:\n",
        "                    preds, p_emb, r_emb = model.forward2(batch_paths_id, batch_relation, paths, paths_time, lengths, path_r,\n",
        "                                                         path_neg_index, batch_his_r)\n",
        "\n",
        "                    del batch_paths_id, batch_relation, paths, paths_time, lengths, path_r, path_neg_index\n",
        "\n",
        "                    loss_e = margin_loss(preds.view(-1), batch_values.view(-1))\n",
        "                    loss_f = cosine_loss(p_emb, r_emb, path_values.view(-1))\n",
        "                    del preds, p_emb, r_emb\n",
        "\n",
        "                    loss = loss_e  # You can add loss_f back if needed\n",
        "\n",
        "                    if torch.isnan(loss):\n",
        "                        print(\n",
        "                            f\"Warning: NaN loss detected at timestep {t}, skipping...\")\n",
        "                        continue\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    batch_count += 1\n",
        "                    current_loss = loss.item()\n",
        "                    epoch_loss.append(current_loss)\n",
        "                    loss_sum += current_loss\n",
        "\n",
        "                    avg_loss = loss_sum / batch_count\n",
        "                    step_time = time.time() - step_start_time\n",
        "\n",
        "                    progress_bar.set_postfix({\n",
        "                        'Loss': f'{current_loss:.4f}',\n",
        "                        'Avg_Loss': f'{avg_loss:.4f}',\n",
        "                        'Step_Time': f'{step_time:.2f}s',\n",
        "                        'Relations': f'{relations_processed}/{total_relations}'\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in forward pass at timestep {t}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if (step_idx + 1) % 10 == 0:\n",
        "                elapsed_time = time.time() - start_time\n",
        "                avg_loss = loss_sum / max(batch_count, 1)\n",
        "                print(f\"\\n  Step {step_idx + 1}/{total_time_steps} | \"\n",
        "                      f\"Avg Loss: {avg_loss:.4f} | \"\n",
        "                      f\"Batches: {batch_count} | \"\n",
        "                      f\"Elapsed: {elapsed_time:.1f}s\")\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        avg_epoch_loss = sum(epoch_loss) / len(epoch_loss) if epoch_loss else 0\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EPOCH {epoch + 1} SUMMARY:\")\n",
        "        print(f\"  Average Loss: {avg_epoch_loss:.6f}\")\n",
        "        print(f\"  Total Batches: {batch_count}\")\n",
        "        print(f\"  Epoch Time: {epoch_time:.1f}s\")\n",
        "        print(f\"  Batches/sec: {batch_count/epoch_time:.2f}\")\n",
        "        print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "        save_model(model, args.data, model_state_file, epoch)\n",
        "\n",
        "        if len(epoch_losses) > 5:\n",
        "            recent_losses = epoch_losses[-5:]\n",
        "            if all(abs(recent_losses[i] - recent_losses[i-1]) < 1e-6 for i in range(1, len(recent_losses))):\n",
        "                print(\"Early stopping: Loss has converged\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Final loss: {epoch_losses[-1]:.6f}\")\n",
        "    return epoch_losses\n",
        "\n",
        "\n",
        "def evaluate_conv(args):\n",
        "    model = TypeGAT(num_e, num_r*2, relation_embeddings, args.embedding_size)\n",
        "    model.load_state_dict(torch.load(\n",
        "        '{0}/trained99.pth'.format(model_state_file), map_location=DEVICE), strict=False)\n",
        "\n",
        "    if HAS_ACCELERATION:\n",
        "        model.to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mr, mrr, hits1, hits3, hits10, hits100 = 0, 0, 0, 0, 0, 0\n",
        "        test_size = 0\n",
        "        graph_test = renamed_load(\n",
        "            open(os.path.join(args.data + '/graph_preprocess_test.pk'), 'rb'))\n",
        "        for t, r_dict in graph_test.t_r_id_p_dict.items():\n",
        "            size = 0\n",
        "            ranks_tail = []\n",
        "            reciprocal_ranks_tail = []\n",
        "            hits_at_100_tail = 0\n",
        "            hits_at_ten_tail = 0\n",
        "            hits_at_three_tail = 0\n",
        "            hits_at_one_tail = 0\n",
        "\n",
        "            for r, id_p in r_dict.items():\n",
        "                for id, ps in id_p.items():\n",
        "                    len_r = 0\n",
        "                    batch_paths_id = []\n",
        "                    batch_relation = []\n",
        "\n",
        "                    batch_his_r = []\n",
        "\n",
        "                    size = size + 1\n",
        "                    len_r = len_r + len(ps)\n",
        "                    batch_paths_id.extend(ps)\n",
        "\n",
        "                    batch_relation.extend([r] * len_r)\n",
        "\n",
        "                    batch_his_r.extend(graph_test.r_copy[t][r][id])\n",
        "\n",
        "                    batch_paths_id = torch.LongTensor(\n",
        "                        list_to_array(batch_paths_id, 0))\n",
        "                    batch_relation = torch.LongTensor(np.array(batch_relation))\n",
        "\n",
        "                    batch_his_r = torch.LongTensor(\n",
        "                        list_to_array(batch_his_r, num_r * 2))\n",
        "\n",
        "                    paths = graph_test.t_paths[t]\n",
        "                    lengths = graph_test.t_paths_len[t]\n",
        "                    paths_time = graph_test.t_paths_time[t]\n",
        "\n",
        "                    if len(paths) != 0:\n",
        "                        paths = pad_sequence([torch.LongTensor(np.array(p)) for p in paths], batch_first=True,\n",
        "                                             padding_value=num_r*2)\n",
        "                        paths_time = pad_sequence([torch.LongTensor(np.array(p)) for p in paths_time], batch_first=True,\n",
        "                                                  padding_value=0)\n",
        "                    else:\n",
        "                        paths = torch.LongTensor(np.array(paths))\n",
        "                        paths_time = torch.LongTensor(np.array(paths_time))\n",
        "                    lengths = torch.LongTensor(np.array(lengths))\n",
        "\n",
        "                    if HAS_ACCELERATION:\n",
        "                        batch_paths_id = Variable(batch_paths_id).to(DEVICE)\n",
        "                        batch_relation = Variable(batch_relation).to(DEVICE)\n",
        "                        paths = Variable(paths).to(DEVICE)\n",
        "                        lengths = Variable(lengths).to(DEVICE)\n",
        "                        paths_time = Variable(paths_time).to(DEVICE)\n",
        "                        batch_his_r = Variable(batch_his_r).to(DEVICE)\n",
        "                    else:\n",
        "                        batch_paths_id = Variable(batch_paths_id)\n",
        "                        batch_relation = Variable(batch_relation)\n",
        "                        paths = Variable(paths)\n",
        "                        lengths = Variable(lengths)\n",
        "                        paths_time = Variable(paths_time)\n",
        "                        batch_his_r = Variable(torch.LongTensor(batch_his_r))\n",
        "\n",
        "                    scores_tail = model.test(\n",
        "                        batch_paths_id, batch_relation, paths, lengths, paths_time, batch_his_r)\n",
        "\n",
        "                    del batch_paths_id, batch_relation, paths, lengths\n",
        "\n",
        "                    sorted_scores_tail, sorted_indices_tail = torch.sort(\n",
        "                        scores_tail.view(-1), dim=-1, descending=True)\n",
        "                    del scores_tail\n",
        "\n",
        "                    # Just search for zeroth index in the sorted scores, we appended valid triple at top\n",
        "                    ranks_tail.append(\n",
        "                        np.where(sorted_indices_tail.cpu().numpy() == 0)[0][0] + 1)\n",
        "                    reciprocal_ranks_tail.append(1.0 / ranks_tail[-1])\n",
        "\n",
        "            for i in range(len(ranks_tail)):\n",
        "                if ranks_tail[i] <= 100:\n",
        "                    hits_at_100_tail = hits_at_100_tail + 1\n",
        "                if ranks_tail[i] <= 10:\n",
        "                    hits_at_ten_tail = hits_at_ten_tail + 1\n",
        "                if ranks_tail[i] <= 3:\n",
        "                    hits_at_three_tail = hits_at_three_tail + 1\n",
        "                if ranks_tail[i] == 1:\n",
        "                    hits_at_one_tail = hits_at_one_tail + 1\n",
        "\n",
        "            assert len(ranks_tail) == len(reciprocal_ranks_tail)\n",
        "            if len(ranks_tail) == 0:\n",
        "                continue\n",
        "\n",
        "            t_hits100 = hits_at_100_tail / len(ranks_tail)\n",
        "            t_hits10 = hits_at_ten_tail / len(ranks_tail)\n",
        "            t_hits3 = hits_at_three_tail / len(ranks_tail)\n",
        "            t_hits1 = hits_at_one_tail / len(ranks_tail)\n",
        "            t_mr = sum(ranks_tail) / len(ranks_tail)\n",
        "            t_mrr = sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail)\n",
        "\n",
        "            print(\"\\nCumulative stats are -> \")\n",
        "            print(\"Hits@100 are {}\".format(t_hits100))\n",
        "            print(\"Hits@10 are {}\".format(t_hits10))\n",
        "            print(\"Hits@3 are {}\".format(t_hits3))\n",
        "            print(\"Hits@1 are {}\".format(t_hits1))\n",
        "            print(\"Mean rank {}\".format(t_mr))\n",
        "            print(\"Mean Reciprocal Rank {}\".format(t_mrr))\n",
        "\n",
        "            test_size = test_size + size\n",
        "\n",
        "            mrr += t_mrr * size\n",
        "            mr += t_mr * size\n",
        "            hits1 += t_hits1 * size\n",
        "            hits3 += t_hits3 * size\n",
        "            hits10 += t_hits10 * size\n",
        "            hits100 += t_hits100 * size\n",
        "\n",
        "        mrr = mrr / test_size\n",
        "        mr = mr / test_size\n",
        "        hits1 = hits1 / test_size\n",
        "        hits3 = hits3 / test_size\n",
        "        hits10 = hits10 / test_size\n",
        "        hits100 = hits100 / test_size\n",
        "\n",
        "        print(\"MR : {:.6f}\".format(mr))\n",
        "        print(\"MRR : {:.6f}\".format(mrr))\n",
        "        print(\"Hits @ 1: {:.6f}\".format(hits1))\n",
        "        print(\"Hits @ 3: {:.6f}\".format(hits3))\n",
        "        print(\"Hits @ 10: {:.6f}\".format(hits10))\n",
        "        print(\"Hits @ 100: {:.6f}\".format(hits100))\n",
        "\n",
        "\n",
        "train_conv(args)\n",
        "evaluate_conv(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File: model_summary.py\n",
        "# Source: ./model_summary.py\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    args = argparse.ArgumentParser()\n",
        "    args.add_argument(\"-data\", \"--data\",\n",
        "                      default=\"./data/ICEWS14_forecasting\", help=\"data directory\")\n",
        "    # Notebook-compatible argument parsing\n",
        "    import sys\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        # Running in notebook - use defaults\n",
        "        args = argparse.Namespace(\n",
        "            data='./data/ICEWS14_forecasting',\n",
        "            state='train',\n",
        "            ratio=1,\n",
        "            his_len=13\n",
        "        )\n",
        "    else:\n",
        "        # Running as script - use command line args\n",
        "        args = args.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "# Notebook-compatible argument setup\n",
        "import sys\n",
        "if 'ipykernel' in sys.modules:\n",
        "    # Running in notebook - create args with defaults\n",
        "    import argparse\n",
        "    args = argparse.Namespace(\n",
        "        data='./data/ICEWS14_forecasting',\n",
        "        state='train',\n",
        "        ratio=1,\n",
        "        his_len=13\n",
        "    )\n",
        "else:\n",
        "    # Running as script - use command line args\n",
        "    args = parse_args()\n",
        "\n",
        "\n",
        "def load_data_info():\n",
        "    data_path = args.data\n",
        "\n",
        "    with open(os.path.join(data_path, 'stat.txt'), 'r') as fr:\n",
        "        for line in fr:\n",
        "            line_split = line.split()\n",
        "            num_e, num_r = int(line_split[0]), int(line_split[1])\n",
        "\n",
        "    return num_e, num_r\n",
        "\n",
        "\n",
        "def print_model_summary():\n",
        "    num_e, num_r = load_data_info()\n",
        "    embedding_size = 200\n",
        "\n",
        "    relation_embeddings = torch.FloatTensor(\n",
        "        np.random.randn(num_r * 2, embedding_size))\n",
        "\n",
        "    model = TypeGAT(num_e, num_r * 2, relation_embeddings, embedding_size)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"MODEL ARCHITECTURE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(model)\n",
        "    print()\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel()\n",
        "                           for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"PARAMETER SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
        "    print()\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"DETAILED PARAMETER BREAKDOWN\")\n",
        "    print(\"=\" * 50)\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name:30} | Shape: {str(param.shape):20} | Params: {param.numel():,}\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\" * 50)\n",
        "    print(\"DATA INFO\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Number of entities: {num_e:,}\")\n",
        "    print(f\"Number of relations: {num_r:,}\")\n",
        "    print(f\"Total relations (with inverse): {num_r * 2:,}\")\n",
        "    print(f\"Embedding dimension: {embedding_size}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print_model_summary()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}